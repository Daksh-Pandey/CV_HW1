{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from loguru import logger\n",
    "from tidecv import TIDE, datasets\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: COCO Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-17 19:14:18.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mLoaded model: YOLOv8x\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.75 ðŸš€ Python-3.10.16 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-PCIE-40GB, 40444MiB)\n",
      "YOLOv8x summary (fused): 268 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ip_arul/daksh21036/CV/HW1/2021036_HW1/data/coco/labels/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [03:37<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5000      36335      0.737      0.647      0.707      0.541\n",
      "                person       2693      10777      0.827      0.769      0.849      0.646\n",
      "               bicycle        149        314      0.732      0.583      0.667      0.431\n",
      "                   car        535       1918      0.759      0.683      0.754       0.54\n",
      "            motorcycle        159        367      0.764      0.714      0.789      0.559\n",
      "              airplane         97        143      0.833      0.874      0.928      0.791\n",
      "                   bus        189        283      0.881      0.813      0.886      0.777\n",
      "                 train        157        190      0.915      0.911      0.956      0.794\n",
      "                 truck        250        414      0.686      0.572      0.652      0.491\n",
      "                  boat        121        424      0.704      0.521      0.597       0.36\n",
      "         traffic light        191        634      0.711      0.509      0.591      0.338\n",
      "          fire hydrant         86        101      0.915      0.851      0.923      0.773\n",
      "             stop sign         69         75      0.807      0.725      0.811      0.734\n",
      "         parking meter         37         60      0.764      0.667       0.72      0.562\n",
      "                 bench        235        411      0.633      0.458      0.497       0.37\n",
      "                  bird        125        427      0.749       0.53      0.635      0.444\n",
      "                   cat        184        202      0.917      0.928      0.959      0.827\n",
      "                   dog        177        218      0.829      0.839      0.861      0.761\n",
      "                 horse        128        272       0.87      0.884      0.923       0.74\n",
      "                 sheep         65        354      0.748      0.822      0.839      0.656\n",
      "                   cow         87        372      0.823      0.801      0.874      0.696\n",
      "              elephant         89        252      0.811       0.92      0.901      0.762\n",
      "                  bear         49         71      0.883      0.915      0.947      0.812\n",
      "                 zebra         85        266      0.847      0.883      0.935      0.782\n",
      "               giraffe        101        232       0.91      0.922      0.952      0.804\n",
      "              backpack        228        371      0.578      0.377      0.411      0.248\n",
      "              umbrella        174        407      0.708      0.697      0.732      0.542\n",
      "               handbag        292        540      0.571       0.37      0.416      0.259\n",
      "                   tie        145        252       0.82       0.58      0.675      0.487\n",
      "              suitcase        105        299      0.725      0.689      0.758      0.548\n",
      "               frisbee         84        115      0.889      0.902      0.927      0.766\n",
      "                  skis        120        241      0.763      0.534      0.605      0.369\n",
      "             snowboard         49         69       0.74      0.577      0.654      0.512\n",
      "           sports ball        169        260      0.797      0.608      0.681      0.513\n",
      "                  kite         91        327      0.695      0.633        0.7      0.519\n",
      "          baseball bat         97        145       0.76       0.69      0.759      0.524\n",
      "        baseball glove        100        148       0.77      0.649      0.722      0.472\n",
      "            skateboard        127        179      0.869      0.849      0.856      0.673\n",
      "             surfboard        149        267      0.838      0.678      0.768      0.535\n",
      "         tennis racket        167        225      0.879      0.876      0.913      0.681\n",
      "                bottle        379       1013      0.721      0.565      0.645      0.472\n",
      "            wine glass        110        341       0.78      0.594       0.68      0.479\n",
      "                   cup        390        895      0.689      0.612      0.697      0.547\n",
      "                  fork        155        215       0.69      0.651      0.712      0.544\n",
      "                 knife        181        325      0.646      0.425        0.5      0.333\n",
      "                 spoon        153        253      0.579      0.419      0.468      0.347\n",
      "                  bowl        314        623       0.67      0.612      0.659      0.526\n",
      "                banana        103        370      0.586      0.374       0.46      0.313\n",
      "                 apple         76        236       0.56      0.329       0.38      0.268\n",
      "              sandwich         98        177      0.655      0.605      0.625      0.497\n",
      "                orange         85        285      0.589      0.372      0.466      0.366\n",
      "              broccoli         71        312      0.575       0.39      0.452      0.283\n",
      "                carrot         81        365      0.513      0.351      0.391      0.266\n",
      "               hot dog         51        125      0.736      0.544      0.638      0.503\n",
      "                 pizza        153        284      0.763      0.736      0.801      0.636\n",
      "                 donut         62        328       0.77      0.583      0.701      0.567\n",
      "                  cake        124        310      0.707      0.603      0.694      0.481\n",
      "                 chair        580       1771      0.682      0.537      0.622      0.439\n",
      "                 couch        195        261       0.65      0.648      0.697      0.556\n",
      "          potted plant        172        342      0.622      0.559      0.586      0.387\n",
      "                   bed        149        163      0.708      0.686      0.751      0.578\n",
      "          dining table        501        695      0.583      0.521      0.534       0.39\n",
      "                toilet        149        179      0.825       0.81      0.878      0.742\n",
      "                    tv        207        288      0.834      0.805      0.858      0.688\n",
      "                laptop        183        231       0.82      0.827      0.875      0.766\n",
      "                 mouse         88        106      0.813      0.811      0.862      0.702\n",
      "                remote        145        283      0.716      0.597      0.675      0.478\n",
      "              keyboard        106        153      0.754      0.693      0.792      0.634\n",
      "            cell phone        214        262      0.721      0.581      0.655      0.479\n",
      "             microwave         54         55       0.75      0.873      0.859      0.716\n",
      "                  oven        115        143      0.686      0.608      0.664      0.501\n",
      "               toaster          8          9      0.519      0.778      0.751      0.545\n",
      "                  sink        187        225      0.661      0.641      0.692      0.495\n",
      "          refrigerator        101        126      0.795      0.794      0.857      0.744\n",
      "                  book        230       1129      0.566      0.185      0.321      0.191\n",
      "                 clock        204        267      0.761       0.73      0.779      0.568\n",
      "                  vase        137        274      0.686      0.588      0.638      0.459\n",
      "              scissors         28         36      0.751      0.472      0.556      0.463\n",
      "            teddy bear         94        190      0.736      0.705      0.773      0.627\n",
      "            hair drier          9         11      0.691      0.182      0.273      0.197\n",
      "            toothbrush         34         57      0.665      0.558      0.611      0.437\n",
      "Speed: 0.3ms preprocess, 8.0ms inference, 0.0ms loss, 6.2ms postprocess per image\n",
      "Saving runs/detect/val/predictions.json...\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-17 19:18:05.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mmean Avg Precision (mAP): 0.5413895196527954\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8x.pt')\n",
    "logger.info('Loaded model: YOLOv8x')\n",
    "metrics = model.val(data='coco_val2017.yaml', save_json=True, device='0')\n",
    "map = metrics.box.map\n",
    "logger.info(f'mean Avg Precision (mAP): {map}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: TIDE Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- coco_predictions --\n",
      "\n",
      "bbox AP @ 50: 9.79\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP    56.72     0.65     0.12     0.08     0.43     0.36  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       4.85       2.51  \n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tide = TIDE()\n",
    "tide.evaluate(datasets.COCO(path='instances_val2017.json'), datasets.COCOResult(path='coco_predictions.json'), \n",
    "              mode=tide.BOX)\n",
    "tide.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Expected Calibration Error (ECE) \n",
    "\n",
    "ECE is a metric to evaluate the calibration of a model. It is defined as the expected value of the absolute difference between the accuracy and the confidence of the model. The confidence of the model is the probability assigned to the predicted class. The accuracy is 1 if the prediction is correct and 0 otherwise. The ECE is computed by dividing the confidence interval [0, 1] into M equally spaced bins and computing the weighted average of the absolute difference between the accuracy and the confidence in each bin. The weights are the proportion of samples in each bin. The ECE is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "ECE = \\sum_{m=1}^{M} \\frac{B_m}{N} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "where $B_m$ is the set of samples in bin $m$, $N$ is the total number of samples, $\\text{acc}(B_m)$ is the accuracy of the model in bin $m$, and $\\text{conf}(B_m)$ is the confidence of the model in bin $m$. The accuracy and confidence in bin $m$ are defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{acc}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\mathbb{1} \\left( y_i = \\hat{y}_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{conf}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} p_i\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the true label of sample $i$, $\\hat{y}_i$ is the predicted label of sample $i$, $p_i$ is the confidence of the model for sample $i$, and $\\mathbb{1}(\\cdot)$ is the indicator function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "\n",
    "The following section contains some starter code to help you prepare the data using the COCO API. You can use this code to preprocess the data, or you can write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Detection:\n",
    "    bbox: np.ndarray  # [x1, y1, x2, y2]\n",
    "    class_id: int\n",
    "    confidence: float\n",
    "\n",
    "@dataclass\n",
    "class GroundTruth:\n",
    "    bbox: np.ndarray  # [x1, y1, x2, y2]\n",
    "    class_id: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_annotations(coco_gt: COCO, coco_dt: COCO, img_id, max_dets=None):\n",
    "    \"\"\"Get list of ground truth and detection annotations across all images\"\"\"\n",
    "    gt_ids = coco_gt.getAnnIds(imgIds=img_id)\n",
    "    gt_anns = coco_gt.loadAnns(gt_ids)\n",
    "    gts = [\n",
    "        GroundTruth(\n",
    "            bbox=ann['bbox'],\n",
    "            class_id=ann['category_id']\n",
    "        )\n",
    "        for ann in gt_anns\n",
    "    ]\n",
    "\n",
    "    # Get detections\n",
    "    dt_ids = coco_dt.getAnnIds(imgIds=img_id)\n",
    "    dt_anns = coco_dt.loadAnns(dt_ids)\n",
    "    if max_dets is not None:\n",
    "        dt_anns = sorted(dt_anns, key=lambda x: x['score'], reverse=True)[:max_dets]\n",
    "    dets = [\n",
    "        Detection(\n",
    "            bbox=ann['bbox'],\n",
    "            class_id=ann['category_id'],\n",
    "            confidence=ann['score']\n",
    "        )\n",
    "        for ann in dt_anns\n",
    "    ]\n",
    "\n",
    "    return gts, dets\n",
    "\n",
    "def prepare_inputs(coco_gt: COCO, coco_dt: COCO, max_dets=100) -> dict:\n",
    "    \"\"\"Convert COCO format to TIDE format for all images\"\"\"\n",
    "    gts_dets = dict()\n",
    "    \n",
    "    for img_id in tqdm(coco_gt.getImgIds(), desc=\"Processing images\"):\n",
    "        gts, dets = get_image_annotations(coco_gt, coco_dt, img_id, max_dets)\n",
    "        if gts and dets:\n",
    "            gts_dets[img_id] = (gts, dets)\n",
    "    \n",
    "    return gts_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=2.31s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "predictions_json = \"coco_predictions.json\"   # Path to the predictions JSON file\n",
    "ground_truth_json = \"instances_val2017.json\"  # Path to the val annotations JSON file\n",
    "\n",
    "coco_gt = COCO(ground_truth_json)       # Load ground truth annotations\n",
    "coco_dt = coco_gt.loadRes(predictions_json) # Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 4642.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gts_dets = prepare_inputs(coco_gt, coco_dt, max_dets=100)\n",
    "len(gts_dets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(bbox1: list, bbox2: list):\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    x = (x1 + w1 - x2) if (x1 <= x2) else (x2 + w2 - x1)\n",
    "    y = (y1 + h1 - y2) if (y1 <= y2) else (y2 + h2 - y1)\n",
    "    intersection = x*y\n",
    "    union = w1*h1 + w2*h2 - intersection\n",
    "    return max(intersection / union, 0)\n",
    "\n",
    "def search_closest_min_idx(confidence: float, m: int):\n",
    "    intervals = np.linspace(0, 1, num=m, endpoint=False)\n",
    "    n = len(intervals)\n",
    "    beg, end = 0, n-1\n",
    "    mid = (beg + end) // 2\n",
    "    while beg <= end:\n",
    "        if (mid == n-1 or intervals[mid] <= confidence < intervals[mid+1]):\n",
    "            break\n",
    "        if confidence <= intervals[mid]:\n",
    "            end = mid\n",
    "        else:\n",
    "            beg = mid + 1\n",
    "        mid = (beg + end) // 2\n",
    "    return mid\n",
    "\n",
    "def calculate_ece(gts_dets: Dict[str, Tuple[List[GroundTruth], List[Detection]]], m: int):\n",
    "    N = len(gts_dets)\n",
    "    proportions = np.zeros(m)\n",
    "    confidences = np.zeros(m)\n",
    "    accuracies = np.zeros(m)\n",
    "    \n",
    "    for gts, dets in gts_dets.values():\n",
    "        for i in range(len(gts)):\n",
    "            max_iou_idx = None\n",
    "            iou = 0\n",
    "            for j in range(len(dets)):\n",
    "                gt_bbox, det_bbox = gts[i].bbox, dets[j].bbox\n",
    "                this_iou = calculate_iou(gt_bbox, det_bbox)\n",
    "                if this_iou > iou:\n",
    "                    max_iou_idx = j\n",
    "            if max_iou_idx is None:\n",
    "                continue\n",
    "            gt, det = gts[i], dets[max_iou_idx]\n",
    "            idx = search_closest_min_idx(det.confidence, m)\n",
    "            confidences[idx] = det.confidence\n",
    "            if gt.class_id == det.class_id:\n",
    "                accuracies[idx] += 1\n",
    "            proportions[idx] += 1\n",
    "    \n",
    "    confidences = np.divide(confidences, proportions, out=np.zeros_like(confidences), where=proportions!=0)\n",
    "    accuracies = np.divide(accuracies, proportions, out=np.zeros_like(accuracies), where=proportions!=0)\n",
    "    ece = 0.0\n",
    "    for i in range(m):\n",
    "        ece += abs(accuracies[i] - confidences[i]) * proportions[i] / N\n",
    "        \n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-20 23:17:39.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mECE: 1.0606147960420031\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'ECE: {calculate_ece(gts_dets, 10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: size-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_THRESH = 32 * 32\n",
    "MEDIUM_THRESH = 96 * 96\n",
    "SMALL_AREA_FILE = 'coco_predictions_small.json'\n",
    "MEDIUM_AREA_FILE = 'coco_predictions_medium.json'\n",
    "LARGE_AREA_FILE = 'coco_predictions_large.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_bboxes_on_scale(gts_dets: Dict[str, Tuple[List[GroundTruth], List[Detection]]]):\n",
    "    small = []\n",
    "    medium = []\n",
    "    large = []\n",
    "    \n",
    "    for img_id, (gts, dets) in gts_dets.items():\n",
    "        for i in range(len(dets)):\n",
    "            det = dets[i]\n",
    "            max_iou_idx = None\n",
    "            iou = 0\n",
    "            for j in range(len(gts)):\n",
    "                det_bbox, gt_bbox = dets[i].bbox, gts[j].bbox\n",
    "                this_iou = calculate_iou(gt_bbox, det_bbox)\n",
    "                if this_iou > iou:\n",
    "                    max_iou_idx = j\n",
    "            if max_iou_idx is None:\n",
    "                continue\n",
    "            gt = gts[max_iou_idx]\n",
    "            area = gt.bbox[2] * gt.bbox[3]\n",
    "            det = asdict(det)\n",
    "            det['image_id'] = img_id\n",
    "            det['category_id'] = det.pop('class_id')\n",
    "            det['bbox'] = list(det['bbox'])\n",
    "            det['score'] = det.pop('confidence')    # ensure proper COCO format\n",
    "            if area <= SMALL_THRESH:\n",
    "                small.append(det)\n",
    "            elif area <= MEDIUM_THRESH:\n",
    "                medium.append(det)\n",
    "            else:\n",
    "                large.append(det)\n",
    "    \n",
    "    json.dump(small, open(SMALL_AREA_FILE, 'w'))\n",
    "    json.dump(medium, open(MEDIUM_AREA_FILE, 'w'))\n",
    "    json.dump(large, open(LARGE_AREA_FILE, 'w'))\n",
    "\n",
    "    logger.info('Saved predictions to JSON files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-18 23:50:32.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclassify_bboxes_on_scale\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mSaved predictions to JSON files\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "classify_bboxes_on_scale(gts_dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-18 23:53:16.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mEvaluating small area predictions\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- coco_predictions_small --\n",
      "\n",
      "bbox AP @ 50: 0.02\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     0.02     0.02     0.00     0.00     0.00     1.69  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       0.01       3.06  \n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tide = TIDE()\n",
    "logger.info('Evaluating small area predictions')\n",
    "tide.evaluate(datasets.COCO(path='instances_val2017.json'), datasets.COCOResult(path=SMALL_AREA_FILE), \n",
    "              mode=tide.BOX)\n",
    "tide.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-18 23:53:24.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mEvaluating medium area predictions\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- coco_predictions_small --\n",
      "\n",
      "bbox AP @ 50: 0.02\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     0.02     0.02     0.00     0.00     0.00     1.69  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       0.01       3.06  \n",
      "=============================\n",
      "\n",
      "-- coco_predictions_medium --\n",
      "\n",
      "bbox AP @ 50: 0.10\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     0.26     0.13     0.00     0.00     0.00     2.76  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       0.06       5.92  \n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Evaluating medium area predictions')\n",
    "tide.evaluate(datasets.COCO(path='instances_val2017.json'), datasets.COCOResult(path=MEDIUM_AREA_FILE), \n",
    "              mode=tide.BOX)\n",
    "tide.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-18 23:53:32.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mEvaluating large area predictions\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- coco_predictions_small --\n",
      "\n",
      "bbox AP @ 50: 0.02\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     0.02     0.02     0.00     0.00     0.00     1.69  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       0.01       3.06  \n",
      "=============================\n",
      "\n",
      "-- coco_predictions_medium --\n",
      "\n",
      "bbox AP @ 50: 0.10\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     0.26     0.13     0.00     0.00     0.00     2.76  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       0.06       5.92  \n",
      "=============================\n",
      "\n",
      "-- coco_predictions_large --\n",
      "\n",
      "bbox AP @ 50: 1.08\n",
      "\n",
      "                         Main Errors\n",
      "=============================================================\n",
      "  Type      Cls      Loc     Both     Dupe      Bkg     Miss  \n",
      "-------------------------------------------------------------\n",
      "   dAP     5.03     3.26     0.02     0.00     0.06     0.71  \n",
      "=============================================================\n",
      "\n",
      "        Special Error\n",
      "=============================\n",
      "  Type   FalsePos   FalseNeg  \n",
      "-----------------------------\n",
      "   dAP       1.83       4.59  \n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Evaluating large area predictions')\n",
    "tide.evaluate(datasets.COCO(path='instances_val2017.json'), datasets.COCOResult(path=LARGE_AREA_FILE), \n",
    "              mode=tide.BOX)\n",
    "tide.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 82789.94it/s]\n",
      "\u001b[32m2025-02-20 23:14:57.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mECE: 0.49465499999999996\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions_small_json = \"coco_predictions_small.json\"   # Path to the predictions JSON file\n",
    "ground_truth_json = \"instances_val2017.json\"  # Path to the val annotations JSON file\n",
    "\n",
    "coco_gt = COCO(ground_truth_json)       # Load ground truth annotations\n",
    "coco_dt = coco_gt.loadRes(predictions_small_json) # Load predictions\n",
    "\n",
    "gts_dets = prepare_inputs(coco_gt, coco_dt)\n",
    "logger.info(f'ECE: {calculate_ece(gts_dets, 10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 83851.53it/s]\n",
      "\u001b[32m2025-02-20 23:15:30.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mECE: 0.49465499999999996\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions_mid_json = \"coco_predictions_medium.json\"   # Path to the predictions JSON file\n",
    "ground_truth_json = \"instances_val2017.json\"  # Path to the val annotations JSON file\n",
    "\n",
    "coco_gt = COCO(ground_truth_json)       # Load ground truth annotations\n",
    "coco_dt = coco_gt.loadRes(predictions_small_json) # Load predictions\n",
    "\n",
    "gts_dets = prepare_inputs(coco_gt, coco_dt)\n",
    "logger.info(f'ECE: {calculate_ece(gts_dets, 10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 4379.18it/s]\n",
      "\u001b[32m2025-02-20 23:16:00.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mECE: 1.002436501616815\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions_large_json = \"coco_predictions_large.json\"   # Path to the predictions JSON file\n",
    "ground_truth_json = \"instances_val2017.json\"  # Path to the val annotations JSON file\n",
    "\n",
    "coco_gt = COCO(ground_truth_json)       # Load ground truth annotations\n",
    "coco_dt = coco_gt.loadRes(predictions_large_json) # Load predictions\n",
    "\n",
    "gts_dets = prepare_inputs(coco_gt, coco_dt)\n",
    "logger.info(f'ECE: {calculate_ece(gts_dets, 10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code: Spatial Grid data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Implementation\n",
    "\n",
    "The `SpatialGrid` class implements a spatial indexing data structure that helps efficiently find nearby objects. Using this data structure is useful for our size-based analysis as it significantly reduces the time complexity of finding overlapping boxes.\n",
    "\n",
    "### How it works:\n",
    "1. The image space is divided into a grid of cells. The size of the cells is defined by the `cell_size` parameter.\n",
    "2. Each bounding box is mapped to the cells it overlaps.\n",
    "3. When searching for nearby objects, we only need to check objects in the relevant grid cells.\n",
    "\n",
    "### Usage Example:\n",
    "```python\n",
    "grid = SpatialGrid(cell_size=100)\n",
    "grid.add_box(0, [100, 100, 200, 200])  # Add a box with index 0, and bbox coordinates [100, 100, 200, 200]\n",
    "nearby = grid.get_nearby_indices([150, 150, 250, 250])  # Find boxes near the bbox [150, 150, 250, 250]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGrid:\n",
    "    def __init__(self, cell_size: int = 100):\n",
    "        \"\"\"A spatial indexing structure for efficient nearby object queries.\n",
    "        \n",
    "        This data structure divides 2D space into a grid and maintains a mapping of\n",
    "        which objects overlap with each grid cell, enabling efficient spatial queries.\n",
    "        \"\"\"\n",
    "        self.cell_size: int = cell_size\n",
    "        # self.grid = defaultdict(list)\n",
    "        self.grid: Dict[Tuple[int, int], List[int]] = defaultdict(list)\n",
    "    \n",
    "    def get_cell_coords(self, bbox: np.ndarray) -> Set[Tuple[int, int]]:\n",
    "        \"\"\"Get all grid cells that a bounding box overlaps with.\n",
    "        \n",
    "        Args:\n",
    "            bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
    "                where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner\n",
    "\n",
    "        Returns:\n",
    "            Set of (x, y) grid cell coordinates\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        start_x = int(x1 // self.cell_size)\n",
    "        end_x = int(x2 // self.cell_size)\n",
    "        start_y = int(y1 // self.cell_size)\n",
    "        end_y = int(y2 // self.cell_size)\n",
    "        return {(i, j) for i in range(start_x, end_x + 1) \n",
    "                       for j in range(start_y, end_y + 1)}\n",
    "    \n",
    "    def add_box(self, idx: int, bbox: np.ndarray):\n",
    "        \"\"\"Add a bounding box to the spatial grid.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the bounding box\n",
    "            bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        cells = self.get_cell_coords(bbox)\n",
    "        for cell in cells:\n",
    "            self.grid[cell].append(idx)\n",
    "    \n",
    "    def get_nearby_indices(self, bbox: np.ndarray) -> Set[int]:\n",
    "        \"\"\"Get indices of bounding boxes that are near the given bounding box.\n",
    "        \n",
    "        Args:\n",
    "            bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
    "            \n",
    "        Returns:\n",
    "            Set of indices of nearby bounding boxes\n",
    "        \"\"\"\n",
    "        cells = self.get_cell_coords(bbox)\n",
    "        nearby = set()\n",
    "        for cell in cells:\n",
    "            nearby.update(self.grid[cell])\n",
    "        return nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size thresholds as per COCO\n",
    "SMALL_THRESH = 32 * 32\n",
    "MEDIUM_THRESH = 96 * 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task: Implement the size-based ECE computation.\n",
    "\n",
    "## Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size-based TIDE computation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
